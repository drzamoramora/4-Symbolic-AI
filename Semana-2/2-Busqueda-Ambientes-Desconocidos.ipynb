{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d5ad706",
   "metadata": {},
   "source": [
    "# Busqueda Offline & Ambientes Desconocidos\n",
    "\n",
    "Hasta ahora nos hemos concentrado en los agentes que usan algoritmos de búsqueda fuera de línea. Calculan una solución completa antes de poner un pie en el mundo real y luego ejecutan la solución.\n",
    "\n",
    "El agente intercala computación y acción: primero realiza una acción, luego observa el entorno y calcula la siguiente acción. La búsqueda en línea es una buena idea en dominios dinámicos o semidinámicos, dominios en los que existe una penalización por quedarse sentado y computar demasiado tiempo.\n",
    "\n",
    "La **búsqueda en línea** es una idea necesaria para **entornos desconocidos**, donde el agente no sabe qué estados existen o qué hacen sus acciones. En este estado de ignorancia, el agente se enfrenta a un problema de **exploración** y debe usar sus acciones como experimentos para aprender lo suficiente como para que la deliberación valga la pena.\n",
    "\n",
    "<img src=\"img/s4.png\" />\n",
    "\n",
    "Un problema de búsqueda en línea debe resolverse mediante la ejecución de acciones por parte de un agente, en lugar de la computación pura. Suponemos un entorno determinista y totalmente observable, pero estipulamos que el agente solo conoce lo siguiente:\n",
    "\n",
    "- Actions(s): retorna una lista de acciones disponibles en el estado s\n",
    "- Costo(s,a,s'): costo de ir de s a s' con la accion a.\n",
    "- Goal-Test(s): se cumple el objetivo en s? \n",
    "\n",
    "Un problema de laberinto simple. El agente comienza en S y debe llegar a G pero no sabe nada del entorno.\n",
    "\n",
    "<img src=\"img/s5.png\" />\n",
    "\n",
    "Dos espacios de estado que podrían llevar a un agente de búsqueda en línea a un callejón sin salida. Cualquier agente fallará en al menos uno de estos espacios\n",
    "\n",
    "<img src=\"img/s6.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b57fc2",
   "metadata": {},
   "source": [
    "## Algoritmos En Linea\n",
    "\n",
    "Un agente en línea recibe una percepción que le dice en qué estado ha llegado; a partir de esta información, puede aumentar su mapa del entorno. El mapa actual se utiliza para decidir adónde ir a continuación. Este intercalado de planificación y acción significa que los algoritmos de búsqueda en línea son bastante diferentes de los algoritmos de búsqueda fuera de línea que hemos visto anteriormente. Por ejemplo, los algoritmos fuera de línea como A∗ pueden expandir un nodo en una parte del espacio y luego expandir inmediatamente un nodo en otra parte del espacio, porque la expansión del nodo implica acciones simuladas en lugar de reales. Un algoritmo en línea, por otro lado, puede descubrir sucesores solo para un nodo que ocupa físicamente. Para evitar viajar por todo el árbol para expandir el siguiente nodo, parece mejor expandir los nodos en un orden local. La búsqueda primero en profundidad tiene exactamente esta propiedad porque (excepto cuando se retrocede) el siguiente nodo expandido es un elemento secundario del nodo anterior expandido\n",
    "\n",
    "### Online DFS\n",
    "\n",
    "<img src=\"img/s7.png\" />\n",
    "\n",
    "Este agente almacena su mapa en una tabla, RESULTADO[s, a], que registra el estado resultante de ejecutar la acción a en el estado s. Siempre que no se haya explorado una acción del estado actual, el agente intenta esa acción. La dificultad viene cuando el agente ha intentado todas las acciones en un estado.\n",
    "\n",
    "### Learning Real-Time A∗ (LRTA*)\n",
    "\n",
    "Construye un mapa del entorno en la tabla de resultados. Actualiza la estimación de costos para el estado que acaba de abandonar y luego elige el movimiento \"aparentemente mejor\" de acuerdo con sus estimaciones de costos actuales. Un detalle importante es que siempre se supone que las acciones que aún no se han probado en un estado s conducen inmediatamente a la meta con el menor costo posible, a saber, h(s). Este optimismo bajo la incertidumbre anima al agente a explorar nuevos caminos, posiblemente prometedores.\n",
    "\n",
    "<img src=\"img/s8.png\" />\n",
    "\n",
    "LRTA*: Este agente selecciona una acción de acuerdo con los valores de los estados vecinos, que se actualizan a medida que el agente se mueve por el espacio de estados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a16352",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
