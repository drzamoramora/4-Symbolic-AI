{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80747ecf-21ef-4281-bb70-b7561ac9bf7e",
   "metadata": {},
   "source": [
    "# Agentes Inteligentes\n",
    "\n",
    "### ¿Que es la inteligencia Artificial\n",
    "\n",
    "#### Actuar Racionalmente\n",
    "\n",
    "* La Inteligencia Computacional es el estudio del diseño de agentes inteligentes. (Poole et al., 1998)\n",
    "\n",
    "### ¿Que es un agentes Inteligente?\n",
    "\n",
    "Un agente es cualquier cosa que pueda verse como que percibe su **entorno** a través de **sensore**s y actúa sobre ese entorno a través de **actuadores**... para maximizar un **objetivo**\n",
    "\n",
    "### Componentes de un Agente:\n",
    "\n",
    "<img src=\"img/agente1.png\" />\n",
    "\n",
    "* **Entorno**: ambiente donde el agente ejecuta su labor\n",
    "* **Sensores**: cualquier tipo de elemento que permita obtener informacion del ambiente u otros agentes\n",
    "* **Actuadores**: cualquier elemento o dispositivo que ejecuta la accion del agente\n",
    "* **Precepto**: entradas del agente en un instante determinado\n",
    "* **Cadena de preceptos**: secuencia de todo lo que agente haya percibido.\n",
    "* **Funcion del agente**: funcion que describe el comportamiento del agente\n",
    "\n",
    "Se dice que la escogencia de la accion de un agente depende de la cadena de preceptos.\n",
    "\n",
    "<img src=\"img/agente2.png\" />\n",
    "<img src=\"img/agente3.png\" />\n",
    "\n",
    "### ¿Que es un agentes racional?\n",
    "\n",
    "Un agente racional es aquel que hace lo correcto; conceptualmente hablando, todas las entradas de la tabla para la función del agente se completan correctamente. Esta noción de deseabilidad es capturada por una medida de desempeño que evalúa cualquier secuencia dada de estados ambientales. Observe que dijimos estados ambientales, no estados de agentes.\n",
    "\n",
    "Esto quiere decir que vamos a establecer una forma evaluar al agente con base a lo que hace en el ambiente. Por ejemplo cuanto polvo puede aspirar por hora, o bien si limpio completamente uno de los espacios A o B. \n",
    "\n",
    "**¿Ud considera que la medida de rendimiento puede causar que el agente haga trampa para aumentar su calificacion final?**\n",
    "\n",
    "### Requerimientos para la Racionalodad de un Agente:\n",
    "\n",
    "* La medida de rendimiento que define el criterio de éxito.\n",
    "* El conocimiento previo del agente sobre el entorno.\n",
    "* Las acciones que puede realizar el agente.\n",
    "* La secuencia de preceptos del agente hasta la fecha.\n",
    "\n",
    "**Ejecicio**: Defina si la aspiradora es un agente racional, verifique si cumple con los requisitos.\n",
    "\n",
    "### Acerca de la autonomia del Agente:\n",
    "\n",
    "* los agentes no siempre conocen el ambiente donde trabajan. Osea no son omniscientes en muchas circumstancias.\n",
    "* los agentes a veces deben navegar el ambiente para recolectar informacion. A esto se le llama exploracion. \n",
    "* el conocimiento del agente puede obtenerse en la exploracion y la ejecucion.\n",
    "* los agentes racionales deberian ser autonomos, es decir, sebe aprender lo que pueda y compensar por lo que no sabe.\n",
    "\n",
    "### Ambientes:\n",
    "\n",
    "Para un agente determinado debemos describirlo en termino de PEAKS (Performance, Environment, Actuators, Sensors)\n",
    "\n",
    "**Ejemplo: Taxi**\n",
    "\n",
    "- Rendimiento: Viaje seguro, rápido, legal, cómodo, maximice las ganancias\n",
    "- Ambiente: Carreteras, otro tráfico, peatones, clientes\n",
    "- Actuadores: Dirección, acelerador, freno, señal, bocina, pantalla\n",
    "- Sensores: Cámaras, sonar, velocímetro, GPS, odómetro, acelerómetro, sensores de motor, teclado\n",
    "\n",
    "**Ejercicio**: Defina el PEAKS para un drone que cuenta frutas en un naranjal.\n",
    "\n",
    "#### Tipos de Ambientes Segun los Sensores:\n",
    "\n",
    "- **Visibilidad - Totalmente Observable**: Si los sensores de un agente le dan acceso al estado completo del entorno en cada momento, entonces decimos que el entorno de la tarea es totalmente observable. Un ejemplo de esto puede ser un juego de space invaders donde el agente sabe donde estan los enemigos, las balas y las barreras.\n",
    "- **Visibilidad - Parcialmente Observable**: Un entorno puede ser parcialmente observable debido a sensores ruidosos e inexactos o porque simplemente faltan partes del estado en los datos del sensor. Un ejemplo de un agente en un ambiente parcialmente-visible puede ser un carro autonomo: este solo puede percibir las cosas mas inmediatas pero ignora muchas cosas que pasan a su alrededor que pueden afectar su ejecucion.\n",
    "- **Visibilidad - No Observable**: cuando el agente no tiene sensores. Esto puede ejemplificarse con un robot tipo Roomba que no tenga sensores, solamente toma desiciones para ir de forma aleatoria y  succionar polvo hasta que se agote la bateria. \n",
    "- **Deterministico o Estocastico**: Si el próximo estado del entorno está completamente determinado por el estado actual y la acción ejecutada por el agente, entonces decimos que el entorno es determinista; de lo contrario, es estocástico. El ejemplo de la aspiradora tiene caracteristicas deterministicas ya que la escogencia del proximo cuadro dependen del estado anterior. El vehiculo autonomo es estocastico ya que lo que va a suceder proximente es inesperado; se puede atravesar un perro, puede aparecer un peaton, etc. \n",
    "- **Episódico o secuencial**: En un entorno de tareas episódicas, la experiencia del agente se divide en episodios atómicos. En cada episodio, el agente recibe una percepción y luego realiza una sola acción. Fundamentalmente, el próximo episodio no depende de las acciones realizadas en episodios anteriores. Muchas tareas de clasificación son episódicas. Por ejemplo, un robot que detecta defectos en una linea de produccion toma desiciones episodicas, no importa si el producto anterior tuvo fallos o no, la nueva evaluacion no contempla las experiencias pasadas. En el caso secuencia, se necesita de todos los preceptos anteriores para tomar una desicion  como en el caso de un agente que juega ajredrez.  \n",
    "- **Estático o dinámico**: Si el entorno puede cambiar mientras un agente está deliberando, entonces decimos que el entorno es dinámico para ese agente; de lo contrario, es estático. En el caso de un vehiculo autonomo, el ambiente esta cambiando constantemente, por tanto se define dinamico. Un juego como sudoku o un crucigrama es estatico. Existe tambien el estado semi-dinamico cuando existe un elemento que cambia algun aspecto del ambiente, pero es algo definido, como la presencia del reloj en el ajedrez.\n",
    "- **Discreto o  continuo**: la distinción discreto/continuo se aplica al estado del entorno, a la forma en que se maneja el tiempo y a las percepciones y acciones del agente. EL ajedrez tiene una naturaleza discreta, se juega paso a paso, una ficha a la vez. Un vehiculo autonomo esta en movimiento, evaluanto el ambiente cada instante por tanto es dinamico. \n",
    "- **Conocido o desconocido**: Estrictamente hablando, esta distinción no se refiere al entorno DESCONOCIDO en sí, sino al estado de conocimiento del agente (o diseñador) sobre las \"leyes de la física\" del entorno. En un entorno conocido, se dan los resultados (o las probabilidades de resultado si el entorno es estocástico) para todas las acciones. Se dice que es conocido si todas las reglas y probabilidades pueden ser estimadas. Es desconocido si no se puede estimar nada. Un juego de Space Invaders es un ambiente conocido. Un carro a autonomo es desconocido.\n",
    "\n",
    "**Un carro autonomo es**: parcialmente observable, multi-agente, estocastico, secuencial, dinamico, continuo y desconocido: esta es posiblemente una de las composiciones ambientales mas dificiles para un agente.\n",
    "\n",
    "<img src=\"img/agente11.png\" />\n",
    "\n",
    "### Estructura Agentes:\n",
    "\n",
    "Ejemplo de Pseudo Codigo de un Agente con tabla de memoria:\n",
    "\n",
    "<img src=\"img/agente4.png\" />\n",
    "\n",
    "\n",
    "### Los 5 Tipos de Agentes:\n",
    "\n",
    "#### Reflex Simple:\n",
    "\n",
    "Estos agentes seleccionan acciones sobre la base de la percepción actual, ignorando el resto del historial de percepción. \n",
    "\n",
    "- Estan llenos de If / Else (reglas condicion-accion)\n",
    "- Pueden tener problemas de ejecucion en ambientes parcialmente observables.\n",
    "- Susceptibles a caer en ciclos. Se pueden usar acciones aleatorias si esto sucede.\n",
    "\n",
    "**Ejemplo para la aspiradora:**\n",
    "\n",
    "<img src=\"img/agente5.png\" />\n",
    "\n",
    "**Pseudo-codigo del patron**\n",
    "\n",
    "<img src=\"img/agente6.png\" />\n",
    "\n",
    "#### Reflex basado en Modelos:\n",
    "\n",
    "La forma más efectiva de manejar la **observabilidad parcial** es que el agente realice un seguimiento de la parte del mundo que no puede ver ahora. Es decir, el agente debe mantener algún tipo de **estado interno** que dependa del historial de percepción y, por lo tanto, refleje al menos algunos de los aspectos no observados del estado actual.\n",
    "\n",
    "- El agente almacena informacion acerca de su entorno.\n",
    "- El agente almacena informacion acerca de sus acciones.\n",
    "- El conjunto de estos datos usados para tomar desiciones se llama **modelo**\n",
    "\n",
    "Ejemplo de una arquitectura basada en modelo:\n",
    "\n",
    "<img src=\"img/agente7.png\" />\n",
    "\n",
    "Algoritmo:\n",
    "\n",
    "<img src=\"img/agente8.png\" />\n",
    "\n",
    "#### Basados en Objetivos (Goals)\n",
    "\n",
    "El agente necesita algún tipo de información de objetivos que describa situaciones que son deseables. El funcionamiento del agente se desarrolla en funcion de maximizar el objetivo. \n",
    "\n",
    "La **búsqueda (Search)** y la **planificación (Planning)** son los subcampos de la IA dedicados a encontrar secuencias de acción que logren los objetivos del agente.\n",
    "\n",
    "El agente tipo Reflex frena cuando ve luces de freno. Un agente basado en objetivos, en principio, podría razonar que si el carro de adelante tiene las luces de freno encendidas, reducirá la velocidad. Dada la forma en que suele evolucionar el mundo, la única acción que logrará el objetivo de no golpear a otros autos es frenar.\n",
    "\n",
    "Por ejemplo, el objetivo de un vehiculo autonomo puede ser ir de A a B. Entonces llegar a B es el objetivo. El agente debe buscar la forma de llegar a B utilizando estrategias de busqueda y planificacion.\n",
    "\n",
    "#### Basados en Utilidades\n",
    "\n",
    "Los objetivos por sí solos no son suficientes para generar un comportamiento de alta calidad en la mayoría de los entornos. La función de utilidad de un agente es esencialmente una internalización de la medida de desempeño. Si la función de utilidad interna y la medida del desempeño externo concuerdan, entonces un agente que elige acciones para maximizar su utilidad será racional de acuerdo con la medida del desempeño externo.\n",
    "\n",
    "Un agente racional basado en la utilidad elige la acción que maximiza la utilidad esperada de los resultados de la acción, es decir, la utilidad que el agente espera obtener, en promedio, dadas las probabilidades y las utilidades de cada resultado.\n",
    "\n",
    "Ejemplo de una arquitectura basada en modelo y utilidad:\n",
    "\n",
    "<img src=\"img/agente9.png\" />\n",
    "\n",
    "Un agente basado en modelos y en utilidade,  utiliza un modelo del mundo, junto con una función de utilidad que mide sus preferencias entre los estados del mundo. Luego, elige la acción que conduce a la mejor utilidad esperada, donde la utilidad esperada se calcula promediando todos los estados de resultado posibles, ponderados por la probabilidad del resultado.\n",
    "\n",
    "\n",
    "#### Basados en Aprendizaje\n",
    "\n",
    "Aprendizaje: permite al agente operar en ambientes inicialmente desconocidos y volverse más competente de lo que su conocimiento inicial podría permitirle.\n",
    "\n",
    "Un agente basado en aprendizaje se basa en 4 elementos: la critica, el elemento de aprendizaje, el elemento de rendimiento y el generador de problemas.\n",
    "\n",
    "<img src=\"img/agente10.png\" />\n",
    "\n",
    "- Elemento de aprendizaje: responsable por realizar mejoras\n",
    "- Elemento de rendimiento: responsable por la seleccion de acciones externas.\n",
    "- Critica: indica el progreso del agente\n",
    "- Generador de problemas: Es responsable de sugerir acciones que conducirán a experiencias nuevas e informativas. El punto es que si el elemento de rendimiento se saliera con la suya, seguiría haciendo las acciones que son mejores, dado lo que sabe. Pero si el agente está dispuesto a explorar un poco y realizar algunas acciones quizás subóptimas a corto plazo, podría descubrir acciones mucho mejores a largo plazo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe228dea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
